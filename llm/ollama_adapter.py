from .llm_adapter import LLMAdapter
from typing import Dict, Any

class OllamaAdapter:
    def __init__(self, model_name: str = "mistral"):
        self.model_name = model_name
        # In echten Projekt: hier Ollama-Client initialisieren

    def generate_harmony(self, prompt: str, context: Dict[str, Any]) -> str:
        """
        Stub: gibt einfach den Prompt zur체ck.
        Sp채ter: Integration mit Ollama-API.
        """
        # F체r MVP nur R체ckgabe des Prompts
        return f"LLM suggestion for prompt: {prompt} (context keys: {list(context.keys())})"
