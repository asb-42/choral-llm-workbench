# Architecture: Vue-based front-end with Ollama LLM

This document outlines the intended architecture to replace the Gradio-based UI with a Vue 3 frontend backed by a NestJS API and a Python core for music processing. Ollama is used as the default local LLM backend with a pluggable adapter to support additional backends in the future. A one-click installer (Pinokio) will bootstrap the whole stack (Python env, Node dependencies, and local LLM binaries).

## Goals
- Local standalone tool to harmonize MusicXML files using a local LLM.
- Separate audible renderings per voice (S, A, T, B) and combined audio.
- Export manipulated scores back to MusicXML.
- Self-contained installation with minimal user interaction.

## Phase 1 Deliverables (Phase 1)
- API surface and data contracts defined for core flows: upload, harmonize, export, and model discovery.
- MVP UI outline for a Vue-based frontend that replaces Gradio, with per-voice prompts and tuning controls.
- Establish a central LLM configuration point and an adapter pattern to support Ollama and future backends.
- Install flow (Pinokio) defined to bootstrap Python, Node, and local LLM binaries with minimal user interaction.

## Stack overview
- Frontend: Vue 3 + Vite + Pinia (UI for upload, prompts per voice, tuning, and per-voice audio previews).
- Backend: NestJS API exposing endpoints for upload, harmonization, analysis, and export; serves as orchestration layer.
- Core: Python-based MusicXML processing (music21), audio rendering (MIDI -> WAV via FluidSynth), and LLM adapters.
- LLM: Ollama backend with an adapter layer to expose a uniform API to the core pipeline.

## Modules and data flow
- MusicXML ingestion (Python core) -> MusicXML score object -> LLM adapter is invoked with per-voice prompts -> LLM returns per-voice chords -> apply chords to measures in the score -> export score (MusicXML) or render audio (WAV) -> frontend can fetch and playback per-voice audio and show visual score changes.

## API surface (high-level)
- POST /api/score/upload: upload MusicXML -> returns scoreId
- POST /api/llm/harmonize: scoreId + prompts -> returns per-voice results
- POST /api/score/export: scoreId + format -> returns exported file
- GET /docs: Swagger UI (auto-generated by NestJS)
- GET /api/llm/models: list available LLM models (for UI population)

## Installation
- Pinokio-based one-click installer will set up Python virtualenv, install Python dependencies, install Node dependencies, and configure a local Ollama model if missing.

## Migration notes
- Gradio UI will be removed in favor of Vue-based frontend.
- Dummy LLM removed; Ollama is the default backend with an adapter.

## MVP API Contracts (Phase 1)
- Data formats:
  - MusicXML as input/output; LLM results in a per-voice mapping as described below.
  - LLM results are represented per voice as: { "measure": int, "root": string, "quality": string }

- Endpoint: POST /api/score/upload
  - Input: multipart/form-data with MusicXML file
  - Response: { "scoreId": string, "parts": number, "measures": number, "scoreInfo": object }

- Endpoint: POST /api/llm/harmonize
  - Input:
    {
      "scoreId": "<id>",
      "llm": {"provider":"ollama","model_name":"<model>"},
      "prompts": {"S":"...","A":"...","T":"...","B":"..."},
      "tuning": <number>,
      "outputs": ["musicxml","wav","midi"]
    }
  - Response:
    {
      "scoreId": "<id>",
      "results": {
        "S": {"measure": 1, "root": "C", "quality": "major"},
        "A": {"measure": 1, "root": "G", "quality": "major"},
        "T": {"measure": 1, "root": "E", "quality": "major"},
        "B": {"measure": 1, "root": "C", "quality": "major"}
      },
      "summary": "Generated via Ollama model ...",
      "exportLinks": {
        "musicxml": "/download/score/<id>.musicxml"
      }
    }

- Endpoint: POST /api/score/export
  - Input: { "scoreId": "<id>", "format": "musicxml" | "midi" | "wav" }
  - Response: { "scoreId": "<id>", "filename": "score_<id>.<ext>", "downloadUrl": "/download/score/<id>.<ext>" }

- Optional: GET /api/llm/models
  - Response: { "models": [{"name":"mistral-7b","provider":"ollama"}, ...] }

## MVP UI (Phase 1)
- Layout: single-page Vue app with sections for file upload, per-voice prompts, tuning, and a results pane.
- Interactions:
  - Upload MusicXML (trigger /api/score/upload)
  - Choose LLM provider/model (default Ollama) and enter per-voice prompts
  - Adjust base tuning (432/440/443)
  - Hit Harmonize (POST /api/llm/harmonize)
  - View updated MusicXML (download) and audio previews (per-voice WAVs, plus combined)

## Migration notes (detailed)
- Remove Gradio-based UI and replace with Vue UI in a staged migration.
- Remove/replace Dummy LLM; UI should select LLM provider (Ollama as default) and fetch models via API.

## UX & Installer Considerations
- Pinokio script will be evolved to provide a seamless One-Click install for non-developers, ensuring that Python, Node, and LLM runtimes are available.
- Provide a minimal, language-agnostic UX (German language prompts as default, with i18n support).
