# Architecture: Vue-based front-end with Ollama LLM

This document outlines the intended architecture to replace the Gradio-based UI with a Vue 3 frontend backed by a NestJS API and a Python core for music processing. Ollama is used as the default local LLM backend with a pluggable adapter to support additional backends in the future. A one-click installer (Pinokio) will bootstrap the whole stack (Python env, Node dependencies, and local LLM binaries).

## Goals
- Local standalone tool to harmonize MusicXML files using a local LLM.
- Separate audible renderings per voice (S, A, T, B) and combined audio.
- Export manipulated scores back to MusicXML.
- Self-contained installation with minimal user interaction.

## Stack overview
- Frontend: Vue 3 + Vite + Pinia (UI for upload, prompts per voice, tuning, and per-voice audio previews).
- Backend: NestJS API exposing endpoints for upload, harmonization, analysis, and export; serves as orchestration layer.
- Core: Python-based MusicXML processing (music21), audio rendering (MIDI -> WAV via FluidSynth), and LLM adapters.
- LLM: Ollama backend with an adapter layer to expose a uniform API to the core pipeline.

## Modules and data flow
- MusicXML ingestion (Python core) -> MusicXML score object -> LLM adapter is invoked with per-voice prompts -> LLM returns per-voice chords -> apply chords to measures in the score -> export score (MusicXML) or render audio (WAV) -> frontend can fetch and playback per-voice audio and show visual score changes.

## API surface (high-level)
- POST /api/score/upload: upload MusicXML -> returns scoreId
- POST /api/llm/harmonize: scoreId + prompts -> returns per-voice results
- POST /api/score/export: scoreId + format -> returns exported file
- GET /docs: Swagger UI (auto-generated by NestJS)

## Installation
- Pinokio-based one-click installer will set up Python virtualenv, install Python dependencies, install Node dependencies, and configure a local Ollama model if missing.

## Migration notes
- Gradio UI will be removed in favor of Vue-based frontend.
- Dummy LLM removed; Ollama is the default backend with an adapter.
